{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9b15b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61502f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "     [0.22, 0.58, 0.33], # with     (x^4)\n",
    "     [0.77, 0.24, 0.10], # one      (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4451883",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beda26e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.6983, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71957035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1456, 0.2281, 0.2251, 0.1287, 0.1066, 0.1658])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the attention scores.\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e45db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1387, 0.2381, 0.2335, 0.1241, 0.1073, 0.1583])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# For normalizing it is adviced to use softmax for smoother gradient flow,\n",
    "# and to make sure scores as alwasy +ve\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights: \", attn_weights_2_naive)\n",
    "print(\"Sum: \", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5f4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "#print(\"Attention weights: \", attn_weights_2)\n",
    "#print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794179c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2_naive = softmax_naive(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196dee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4416, 0.6508, 0.5687])\n"
     ]
    }
   ],
   "source": [
    "# computing context vectors. \n",
    "# Multiplying attention weights to token and summing them up\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aed9f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4561, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.6983, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7069, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3416, 0.6565],\n",
      "        [0.4561, 0.6983, 0.7069, 0.3416, 0.6605, 0.2855],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2855, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Attn scores for all the tokens\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab4ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn scores:\n",
      " tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4561, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.6983, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7069, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3416, 0.6565],\n",
      "        [0.4561, 0.6983, 0.7069, 0.3416, 0.6605, 0.2855],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2855, 0.9450]]) \n",
      "Attn weights:\n",
      "  tensor([[0.2099, 0.2006, 0.1982, 0.1243, 0.1219, 0.1452],\n",
      "        [0.1387, 0.2381, 0.2335, 0.1241, 0.1073, 0.1583],\n",
      "        [0.1391, 0.2371, 0.2328, 0.1243, 0.1100, 0.1566],\n",
      "        [0.1436, 0.2075, 0.2047, 0.1463, 0.1257, 0.1722],\n",
      "        [0.1534, 0.1954, 0.1971, 0.1368, 0.1881, 0.1293],\n",
      "        [0.1386, 0.2185, 0.2129, 0.1422, 0.0981, 0.1897]])\n"
     ]
    }
   ],
   "source": [
    "# with matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(\"Attn scores:\\n\", attn_scores,\"\\nAttn weights:\\n \", torch.softmax(attn_scores, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef10b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2099, 0.2006, 0.1982, 0.1243, 0.1219, 0.1452],\n",
      "        [0.1387, 0.2381, 0.2335, 0.1241, 0.1073, 0.1583],\n",
      "        [0.1391, 0.2371, 0.2328, 0.1243, 0.1100, 0.1566],\n",
      "        [0.1436, 0.2075, 0.2047, 0.1463, 0.1257, 0.1722],\n",
      "        [0.1534, 0.1954, 0.1971, 0.1368, 0.1881, 0.1293],\n",
      "        [0.1386, 0.2185, 0.2129, 0.1422, 0.0981, 0.1897]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2efee8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4420, 0.5919, 0.5791],\n",
      "        [0.4416, 0.6508, 0.5687],\n",
      "        [0.4428, 0.6489, 0.5675],\n",
      "        [0.4301, 0.6288, 0.5514],\n",
      "        [0.4671, 0.5884, 0.5266],\n",
      "        [0.4174, 0.6497, 0.5649]])\n"
     ]
    }
   ],
   "source": [
    "# compute all context vectors\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cb0cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous context vector:  tensor([0.4416, 0.6508, 0.5687])\n"
     ]
    }
   ],
   "source": [
    "print(\"previous context vector: \", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c0df1",
   "metadata": {},
   "source": [
    "### Self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "423233e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff0481cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a5120d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bc6e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d7e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys:  tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1809, 0.3220],\n",
      "        [0.3275, 0.9642]])\n",
      "values:  tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1460, 0.3306],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys: \", keys)\n",
    "print(\"values: \", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "790750b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "print(attn_scores_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "718cf519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5464, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f1ff09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1501, 0.2265, 0.2200, 0.1312, 0.0900, 0.1822])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "227329d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3059, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c03719",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact self-attention python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c09a0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660f5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5cd4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2993, 0.8052],\n",
      "        [0.3059, 0.8210],\n",
      "        [0.3056, 0.8203],\n",
      "        [0.2945, 0.7936],\n",
      "        [0.2922, 0.7885],\n",
      "        [0.2988, 0.8039]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff3166",
   "metadata": {},
   "source": [
    "We can improve the `SelfAttention_v1` implementation further by utilizing\n",
    "PyTorch’s `nn.Linear` layers, which effectively perform matrix multiplication when\n",
    "the bias units are disabled. Additionally, a significant advantage of using `nn.Linear` \n",
    "instead of manually implementing `nn.Parameter(torch.rand(...))` is that `nn.Linear`\n",
    "has an optimized weight initialization scheme, contributing to more stable and\n",
    "effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f2f5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93eaeca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5332, -0.1044],\n",
      "        [-0.5318, -0.1073],\n",
      "        [-0.5318, -0.1072],\n",
      "        [-0.5291, -0.1069],\n",
      "        [-0.5305, -0.1058],\n",
      "        [-0.5293, -0.1073]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "580280f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]], requires_grad=True)\n",
      "W_key Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]], requires_grad=True)\n",
      "W_value Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, parameters in sa_v1.named_parameters():\n",
    "    print(name, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf2b3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = sa_v1.get_parameter('W_query')\n",
    "k = sa_v1.get_parameter('W_key')\n",
    "v = sa_v1.get_parameter('W_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ff67bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v2.W_query.weight = nn.Parameter(q.T)\n",
    "sa_v2.W_key.weight = nn.Parameter(k.T)\n",
    "sa_v2.W_value.weight = nn.Parameter(v.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddec641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.weight Parameter containing:\n",
      "tensor([[0.2961, 0.2517, 0.0740],\n",
      "        [0.5166, 0.6886, 0.8665]], requires_grad=True)\n",
      "W_key.weight Parameter containing:\n",
      "tensor([[0.1366, 0.1841, 0.3153],\n",
      "        [0.1025, 0.7264, 0.6871]], requires_grad=True)\n",
      "W_value.weight Parameter containing:\n",
      "tensor([[0.0756, 0.3164, 0.1186],\n",
      "        [0.1966, 0.4017, 0.8274]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, parameters in sa_v2.named_parameters():\n",
    "    print(name, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef38631",
   "metadata": {},
   "source": [
    "### 3.5 Hiding future words with `causal attention`\n",
    "\n",
    "  1. Take attention score (unnormalized).\n",
    "  2. Apply `softmax` to get attention \"weights\" (normalized).\n",
    "  3. Mask with 0's above diagonal to get masked attention scores (unnormalized).\n",
    "  4. Normalize the row to get masked attention \"weights\" (nomralized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09e65be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1552, 0.2106, 0.2061, 0.1414, 0.1068, 0.1800],\n",
      "        [0.1501, 0.2265, 0.2200, 0.1312, 0.0900, 0.1822],\n",
      "        [0.1505, 0.2258, 0.2194, 0.1316, 0.0908, 0.1820],\n",
      "        [0.1592, 0.1995, 0.1963, 0.1478, 0.1202, 0.1770],\n",
      "        [0.1612, 0.1947, 0.1921, 0.1503, 0.1265, 0.1752],\n",
      "        [0.1558, 0.2093, 0.2050, 0.1420, 0.1084, 0.1795]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries =sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cf012",
   "metadata": {},
   "source": [
    "We can implement the second step using PyTorch’s `tril` function to create a mask\n",
    "where the values above the diagonal are zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a2ead17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e8cfe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1552, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1501, 0.2265, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1505, 0.2258, 0.2194, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1592, 0.1995, 0.1963, 0.1478, 0.0000, 0.0000],\n",
      "        [0.1612, 0.1947, 0.1921, 0.1503, 0.1265, 0.0000],\n",
      "        [0.1558, 0.2093, 0.2050, 0.1420, 0.1084, 0.1795]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56f25586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
      "        [0.1954, 0.2361, 0.2329, 0.1823, 0.1533, 0.0000],\n",
      "        [0.1558, 0.2093, 0.2050, 0.1420, 0.1084, 0.1795]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Renormalizing\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d0a44",
   "metadata": {},
   "source": [
    "A more efficient way to obtain the masked attention weight matrix in\n",
    "causal attention is to mask the attention scores with negative infinity values before\n",
    "applying the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87d1d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2705, 1.8524,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.2544, 1.8284, 1.7877,   -inf,   -inf,   -inf],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925,   -inf,   -inf],\n",
      "        [0.6052, 0.8730, 0.8538, 0.5069, 0.2627,   -inf],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3856, 1.0996]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42eb2e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3986, 0.6014, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2526, 0.3791, 0.3683, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2265, 0.2839, 0.2794, 0.2103, 0.0000, 0.0000],\n",
      "        [0.1954, 0.2361, 0.2329, 0.1823, 0.1533, 0.0000],\n",
      "        [0.1558, 0.2093, 0.2050, 0.1420, 0.1084, 0.1795]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca8a6d",
   "metadata": {},
   "source": [
    "In the following code example, we use a dropout rate of 50%, which means masking <br>\n",
    "out half of the attention weights. (When we train the GPT model in later chapters, <br>\n",
    "we will use a lower dropout rate, such as 0.1 or 0.2.) We apply PyTorch’s dropout <br>\n",
    "implementation first to a 6 × 6 tensor consisting of 1s for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67e26fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe42c9",
   "metadata": {},
   "source": [
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. To compensate for the reduction in\n",
    "active elements, the values of the remaining elements in the matrix are scaled up by a\n",
    "factor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains\n",
    "consistent during both the training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "235a1996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2029, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7366, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4529, 0.5677, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3908, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4186, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55e760",
   "metadata": {},
   "source": [
    "### 3.5.3 Implementing a compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f14663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f8cc1344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2400, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2400, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb97cb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
       "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2400, 0.8000],\n",
       "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
       "         [0.1500, 0.8700, 0.8500, 0.5800, 0.2400, 0.8000],\n",
       "         [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "25e66f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out   = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "    \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "    \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "27d9e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2cbd1a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5519, -0.0972],\n",
       "         [-0.5293, -0.1073]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5519, -0.0972],\n",
       "         [-0.5293, -0.1073]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621d424",
   "metadata": {},
   "source": [
    "### 3.6 Single head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6fcbe",
   "metadata": {},
   "source": [
    "We will tackle this expansion from causal attention to multi-head attention. First, <br>\n",
    "we will intuitively build a multi-head attention module by stacking multiple Causal <br>\n",
    "Attention modules. Then we will then implement the same multi-head attention <br>\n",
    "module in a more complicated but more computationally efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c77bdb",
   "metadata": {},
   "source": [
    "#### Stacking multiple single-headed attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ae8dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2e88c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6f45c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1471,  0.4106,  0.4675, -0.2793],\n",
      "         [-0.2493,  0.3548,  0.4651, -0.0590],\n",
      "         [-0.2782,  0.3323,  0.4578,  0.0089],\n",
      "         [-0.2636,  0.2932,  0.4108,  0.0479],\n",
      "         [-0.2190,  0.2184,  0.3162,  0.0206],\n",
      "         [-0.2415,  0.2433,  0.3491,  0.0629]],\n",
      "\n",
      "        [[-0.1471,  0.4106,  0.4675, -0.2793],\n",
      "         [-0.2493,  0.3548,  0.4651, -0.0590],\n",
      "         [-0.2782,  0.3323,  0.4578,  0.0089],\n",
      "         [-0.2636,  0.2932,  0.4108,  0.0479],\n",
      "         [-0.2190,  0.2184,  0.3162,  0.0206],\n",
      "         [-0.2415,  0.2433,  0.3491,  0.0629]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape:  torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape: \", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3938852",
   "metadata": {},
   "source": [
    "Instead of maintaining two separate classes, `MultiHeadAttentionWrapper` and <br>\n",
    "`CausalAttention` , we can combine these concepts into a single `MultiHeadAttention` <br>\n",
    "class. Also, in addition to merging the `MultiHeadAttentionWrapper` with the Causal <br>\n",
    "Attention code, we will make some other modifications to implement multi-head <br>\n",
    "attention more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "66c32f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following MultiHeadAttention class integrates the multi-head functionality within a single class.\n",
    "# It splits the input into multiple heads by reshaping the projected query, key, and value\n",
    "# tensors and then combines the results from these heads after computing attention.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                    context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\",\n",
    "                             torch.triu(torch.ones(context_length, context_length),\n",
    "                                        diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # We implicitly split the matrix by adding a num_heads\n",
    "        # dimension. Then we unroll the last dim: \n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transposes from shape (b, num_tokens, num_heads, head_dim) to \n",
    "        # (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # Computes dot product for each head\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        \n",
    "        # mask truncated to the number of tokens\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # uses mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores /keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # Tensor shape: (b, num_tokens, num_heads, head_dim)\n",
    "        \n",
    "        # Combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec) # Adds an additional linear projection\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bf30675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d9d172d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3b7cf35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "print(a @ a.transpose(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6680f8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"Second head:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9f9c17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 4\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3e19411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.rand([2, 1024, 768])\n",
    "mha_gpt = MultiHeadAttention(768, 768, 1024, 0.0, num_heads=12)\n",
    "context_vec_gpt2 = mha_gpt(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
